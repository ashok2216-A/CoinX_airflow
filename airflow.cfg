# [core]
# # The home directory for Airflow. All paths are relative to this directory.
# airflow_home = /usr/local/airflow

# # The folder where your DAGs are stored.
# dags_folder = /usr/local/airflow/dags

# # The folder where your logs are stored.
# base_log_folder = /usr/local/airflow/logs

# # The SQL Alchemy connection string for your metadata database.
# sql_alchemy_conn = postgresql+psycopg2://username:password@host:port/database

# # The executor type to use. Options are:
# # - SequentialExecutor (default)
# # - LocalExecutor
# # - CeleryExecutor
# # - DaskExecutor
# # - KubernetesExecutor
# executor = LocalExecutor

# # Whether to load example DAGs (set to False to disable example DAGs).
# load_examples = False

# # The timezone to use for scheduling tasks.
# default_timezone = UTC

# # The default number of retries for tasks.
# default_task_retries = 3

# # The maximum number of active DAG runs.
# max_active_runs_per_dag = 16

# [webserver]
# # The port on which the web server will run.
# web_server_port = 8080

# # Whether to enable authentication for the web UI.
# web_server_authenticate = False

# # The web server's secret key used for securing sessions.
# secret_key = temporary_key_for_security

# # The base URL for the web server.
# base_url = http://localhost:8080

# [scheduler]
# # The interval (in seconds) at which the scheduler should run.
# scheduler_task_queued_timeout = 600

# # The maximum number of concurrently running tasks for the scheduler.
# scheduler_task_queued_max = 128

# # The number of worker processes to spawn.
# scheduler_workers = 4

# [logging]
# # The logging level for Airflow.
# logging_level = INFO

# # The log format for Airflow logs.
# log_format = %(asctime)s - %(name)s - %(levelname)s - %(message)s

# # The log format for task logs.
# task_log_format = %(asctime)s - %(name)s - %(levelname)s - %(message)s

# [celery]
# # Celery configuration for distributed task execution.
# # If using CeleryExecutor, set up Celery's broker URL.
# broker_url = redis://localhost:6379/0

# # The result backend for Celery.
# result_backend = redis://localhost:6379/0

# [metrics]
# # Whether to enable metrics collection (default is False).
# statsd_on = False

# # The statsd host to send metrics to.
# statsd_host = localhost

# [database]
# # Whether to enable the database migrations (default is True).
# migrate = True

# # The database connection string for Airflow's metadata database.
# sql_alchemy_conn = postgresql+psycopg2://username:password@host:port/database
